{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5e1f4b-cf2b-4780-b8fe-25520e5c4d4a",
   "metadata": {},
   "source": [
    "# Real-world Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02209ae-4da9-4bde-bd3e-ecb28f531338",
   "metadata": {},
   "source": [
    "## 1. Gather data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe22f4f9-3fc8-4753-a66a-31d09e74ad86",
   "metadata": {},
   "source": [
    "I will be using the **YouTube Top ~5000 Channel IDs** dataset  for this project, which can be found in the Kaggle website (https://www.kaggle.com/datasets/amirmasoud32/youtube-top-5000-channel-ids). In addition, I will be gathering data programmatically using the YouTube API for the corresponding channels in the first dataset.\n",
    "\n",
    "### **1.1.** Problem Statement\n",
    "The datset selected for this project, centers around the user parameters for Youtube channels. Therefore, we should be able to dig into information regarding the user counts for individual channels, details related to vedios present and number of views in those channels.\n",
    "\n",
    "Problems I will be looking at are;\n",
    "1. Which channels have most reported subscribers?\n",
    "2. Are there any relashionships between parameters such as, published date, total number of vedios, number of subscribers and the total views for these channels?\n",
    "\n",
    "Befor begining the project it is essential to load the package to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ba1672-a4a5-423e-9efd-7c843959fb20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4d3c7-bdf2-40b0-9948-29ac0ad23ea8",
   "metadata": {},
   "source": [
    "### **1.2.** Gather data from Dataset 1\n",
    "\n",
    "#### **Dataset 1**: YouTube Top ~5000 Channel IDs dataset \n",
    "\n",
    "I find it quite intesresting to get to know more details about the Youtube channels available and their popularity. Hence, I chose this dataset for the project.  \n",
    "\n",
    "Type: Zipped .csv File\n",
    "\n",
    "Method: The data was gathered using the \"Downloading files\" method from Kaggle\n",
    "\n",
    "Dataset variables:\n",
    "\n",
    "* Name: Name of the channel\n",
    "* ID: Channel ID\n",
    "\n",
    "For Dataset 1, we first need to unzip the file and then read the.csv file. Afterwards, we can access the data in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70740ef0-258d-49d0-a624-2e5a04dcd4ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# unzip the zip file in read mode using a context manager\n",
    "with zipfile.ZipFile(\"Youtube.zip\",\"r\") as zip_ref: \n",
    "    zip_ref.extractall(\"Youtube/\")\n",
    "    \n",
    "# load a csv into a pandas dataframe\n",
    "df = pd.read_csv('Youtube/output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48127534-d14f-4989-bffa-d2d5e798d860",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zee TV</td>\n",
       "      <td>UCppHT7SZKKvar4Oc9J4oljQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T-Series</td>\n",
       "      <td>UCq-Fj5jknLsUf-MWSy4_brA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name                        ID\n",
       "0    Zee TV  UCppHT7SZKKvar4Oc9J4oljQ\n",
       "1  T-Series  UCq-Fj5jknLsUf-MWSy4_brA"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the data frame\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144e09f-172d-4a84-b335-1f6444099250",
   "metadata": {},
   "source": [
    "### **1.3.** Gather data from Dataset 2\n",
    "#### Dataset 2 : Gather data by accessing Youtube API\n",
    "\n",
    "Why picked the dataset? I selected this data gathering method to complement well with the first dataset I chose previously. Data gathering using this methods enables to gather more useful details to be used for the analysis.\n",
    "\n",
    "Type: Gather data by accessing APIs\n",
    "\n",
    "Method: The data was gathered using the \"API\" method from Youtube (https://developers.google.com/youtube/v3/docs/comments/list).\n",
    "\n",
    "Dataset variables:\n",
    "\n",
    "* ID : Channel ID for the Youtube channel            \n",
    "* Subscribers : Number of subscribers for the channel\n",
    "* Total_vedios : Total number of vedios in the channel \n",
    "* Views : Total number of views for the channel         \n",
    "* Published Date :  Date the channel was created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c99617d",
   "metadata": {},
   "source": [
    "The first step will be to install Google API client and Google authentication library in order to retreive the related channel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db0dc7-d107-4081-8426-e911ce13d74a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install Google APIs Client Library for Python\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade google-api-python-client\n",
    "\n",
    "# install libraries for user authorization\n",
    "!{sys.executable} -m pip install --upgrade google-auth-oauthlib google-auth-httplib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c442b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import above installed packages\n",
    "import os\n",
    "\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "\n",
    "scopes = [\"https://www.googleapis.com/auth/youtube.readonly\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6952c5",
   "metadata": {},
   "source": [
    "Next the authorization credentials were generated as an API key at the Google Developers Console (https://console.cloud.google.com/apis/dashboard?project=project3-433207). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f54fb6-b579-4fe2-a81e-432ae780a4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define API key\n",
    "API_key = 'AIzaSyCWFU0TVY8o2Lw53mhCb00JUsB_9tQ0464'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea24994-bece-4f10-a0cd-518172f56cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# define api_service_name and api_version\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "\n",
    "# create an API client\n",
    "youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, developerKey=API_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b436a",
   "metadata": {},
   "source": [
    "After the google API service name and version are defined, and the API client is created we need to extract the chennel ID information from the first dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b7709a-a86d-4312-a458-418e80c89a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract channel ID information from the database df\n",
    "channel_ids_all = df.iloc[:, 1].tolist()\n",
    "\n",
    "# test\n",
    "print(len(channel_ids_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad880e7",
   "metadata": {},
   "source": [
    "We create the function below to fetch the data from Youtube API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff40a1f-91dd-4ba9-ac5a-b1821fdad229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define fetch_stats function\n",
    "def fetch_stats(youtube, channel_ids):\n",
    "\n",
    "    all_data = []\n",
    "    #print(channel_ids)\n",
    "    request = youtube.channels().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        id=','.join(channel_ids),\n",
    "        maxResults = 50,\n",
    "    )\n",
    "    \n",
    "    response = request.execute()\n",
    "    \n",
    "    # add data into dictionaries\n",
    "    for x in response['items']:\n",
    "        data = {'ID': x['id'],\n",
    "               # 'Description': x['snippet']['description'],\n",
    "                'Subscribers': x['statistics']['subscriberCount'],\n",
    "                'Total_vedios': x['statistics']['videoCount'],\n",
    "                'Views': x['statistics']['viewCount'],\n",
    "                'Published_date': x['snippet']['publishedAt']\n",
    "                }\n",
    "    \n",
    "        all_data.append(data)\n",
    "\n",
    "    return(pd.DataFrame(all_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a8b074",
   "metadata": {},
   "source": [
    "Then we call the above defined function is blocks of 50 till all the data from the first dataset is passed to the function to extract information. We pass the blocks in 50 entries each as this is the  maximum number of items that  Youtube allows to be returned in a single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2298f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create results list\n",
    "results = []\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 50\n",
    "\n",
    "# Loop through the list in chunks\n",
    "for i in range(0, len(channel_ids_all), chunk_size): \n",
    "    chunk = channel_ids_all[i:i + chunk_size]\n",
    "    result_df = fetch_stats(youtube, chunk)\n",
    "    #print(len(result_df))\n",
    "    results.append(result_df)\n",
    "    \n",
    "# Combine all DataFrames into a single DataFrame\n",
    "stats_df = pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1b2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving raw data gathered from API\n",
    "stats_df.to_csv('API_gathered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0031f6-2604-49b5-b443-569e0a2612a6",
   "metadata": {},
   "source": [
    "## 2. Assess data\n",
    "\n",
    "Next we assess the data to elavuate for any data quality and tidiness issues with those matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d40e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into the first few lines of df dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def26433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  check information about df dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into the first few lines of stats_df dataframe\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d77b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  check information about df dataframe\n",
    "stats_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f1921",
   "metadata": {},
   "source": [
    "From the above few steps we are able to get a brief knowledge about what kind of dataset we are looking at, which is helpful in the next stage of data assessing to find quality and tidiness issues. Afterwards, we Check for NaN values in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8232719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91bc52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "stats_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca282ab",
   "metadata": {},
   "source": [
    "Both datasets have no null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0097a942-6dba-4297-894d-46e6951c1fb8",
   "metadata": {},
   "source": [
    "### Quality Issue 1:\n",
    "\n",
    "**Duplicate entires in both datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55da4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicated entries in the df dataset visually\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de0b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicated entries in the ID column visually\n",
    "stats_df['ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4ff285",
   "metadata": {},
   "source": [
    "`.nunique()` provides the opportunity to find how many unique entries are present in each column of the dataframe. Since there are no NaN values in both data frames we are working with, out of the 4998 entries in the df dataframe there must be 7 and 78 duplicated entried in `Name` and `ID` columns, and out of the 4612 entries in the stats_df dataframe there must be 72 duplicate entries present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e31ed-15cd-4a08-bd48-169d794c2c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the dataframe programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3798082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicated entries in the Name column\n",
    "df[df.duplicated(subset='Name')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f70b5a",
   "metadata": {},
   "source": [
    "Due to some reason the `ID` column in the df dataset cannot be called simply by using its name, `ID`. Therefore, we need to call the column by its index as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad7956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID column has index '1'\n",
    "index_ID = 1\n",
    "\n",
    "# Get column name using iloc\n",
    "column_name_to_check = df.columns[index_ID]\n",
    "\n",
    "# Find duplicated entries in the ID column\n",
    "df[df.duplicated(subset=[column_name_to_check])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicated rows in the stats_df dataset\n",
    "stats_df[stats_df.duplicated(subset=['ID'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6d3e3e-97a0-4928-be6c-80e566b3a744",
   "metadata": {},
   "source": [
    "Issue and justification: \n",
    "\n",
    "As pointed out at the begining of this section, both data frames have duplicated entries present in them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d96f95",
   "metadata": {},
   "source": [
    "### Quality Issue 2:\n",
    "\n",
    "**Non returning values for the 2nd dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e9a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the dataframe visually\n",
    "\n",
    "# Find duplicated entries in the df dataset visually\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b06535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicated entries in the ID column visually\n",
    "stats_df['ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c0151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the dataframe programmatically\n",
    "stats_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29179b21-ba3b-4e78-8147-979f17ea1b5e",
   "metadata": {},
   "source": [
    "Issue and justification: \n",
    "\n",
    "Even if we consider the 2 datasets without the duplicated entries, it can be seen that the API is not returning data for all the entries in the df dataframe entires i.e. for all the 4920 unique ID values in df. The API only returns values for 4540 entries. Therefore, there are 380 entries in the original dataset which doesn't have extracted results from the API calling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88785bb2-5418-4d2c-b8c0-36a6cbf421f0",
   "metadata": {},
   "source": [
    "### Tidiness Issue 1: \n",
    "**Formatting issue in ID column for df dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7edcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the dataframe visually\n",
    "\n",
    "# check the column details for df\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7f8ac",
   "metadata": {},
   "source": [
    "As seen in the table above the the ID tag is followed by some unseen characters when displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb47922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the dataframe programmatically\n",
    "\n",
    "# Define the column name you want to check\n",
    "check_column = df.columns[1]\n",
    "\n",
    "if check_column == \"ID\":\n",
    "    print(f\"Column ID is present in DataFrame.\")\n",
    "else:  \n",
    "    print(f\"Column ID not found in DataFrame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cc4e80-d944-4ee3-9f43-6768be83b6c6",
   "metadata": {},
   "source": [
    "Issue and justification: \n",
    "\n",
    "The `ID` heading in `df` dataframe has a formetting difference. Hence it cannot be called using just by entering the name. It could be because there are some unseen characters together with its name included when creating the dataframe. This issue is difficult to inspect visually but gets and error when the column is called referring to its name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16716858-c505-469b-b95c-0e2962b4db6a",
   "metadata": {},
   "source": [
    "### Tidiness Issue 2:\n",
    "**Published Date is of object data type and has both date and time stamps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddc93c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspecting the dataframe visually\n",
    "\n",
    "# investigate the stats_df dataframe \n",
    "stats_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2747d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive info from starts_df\n",
    "stats_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab26094-5d0f-4a97-a0c6-95f74775601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the dataframe programmatically\n",
    "\n",
    "# Check if data type is object\n",
    "assert stats_df.Published_date.dtype == 'object'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd580c-6ff1-4272-9bed-166d344559cf",
   "metadata": {},
   "source": [
    "Issue and justification: \n",
    "\n",
    "The `Published_date` column entries are in the object data type and in a format representing both the date and the time in each of the entries. It will be more benefial to have the date and time separated in the `Published_date`column. For further analysis we will need to convert the Published_date column into `datetime` format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a1522-6c98-448d-a487-1fef0fe0f88f",
   "metadata": {},
   "source": [
    "## 3. Clean data\n",
    "The first step in the cleaning stage is to make copies of the data set to ensure the raw dataframes are not impacted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc250dd",
   "metadata": {},
   "source": [
    "Clean the data to solve the 4 issues corresponding to data quality and tidiness found in the assessing step. **Make sure you include justifications for your cleaning decisions.**\n",
    "\n",
    "After the cleaning for each issue, please use **either** the visually or programatical method to validate the cleaning was succesful.\n",
    "\n",
    "At this stage, you are also expected to remove variables that are unnecessary for your analysis and combine your datasets. Depending on your datasets, you may choose to perform variable combination and elimination before or after the cleaning stage. Your dataset must have **at least** 4 variables after combining the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47cf348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies of the datasets\n",
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f95f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies of the datasets\n",
    "stats_df_clean = stats_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9290e1",
   "metadata": {},
   "source": [
    "### Tidiness Issue 1: \n",
    "**Formatting issue in ID column for df dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9037dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning strategy: Rename column ID in df dataset\n",
    "\n",
    "# Get the current column names\n",
    "columns = df_clean.columns.tolist()\n",
    "\n",
    "# Create a dictionary for renaming\n",
    "rename_dict = {columns[1]: 'ID'}\n",
    "\n",
    "# Rename the column using the rename method\n",
    "df_clean.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# Validate the cleaning was successful\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8374b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the cleaning was successful\n",
    "df_clean.ID.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac362d68",
   "metadata": {},
   "source": [
    "Justification:\n",
    "\n",
    "Due to a formatting issue the `ID` column name in the df dataframe was not callable by its name. Therefore, it was necessary to rename the column. After the renaming now it can be seen that the issue is resolved and the clomun can be called using `ID` label. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378708ef-b843-4f1b-bfff-ef673e790150",
   "metadata": {},
   "source": [
    "### Quality Issue 2: \n",
    "**Drop duplicated rows in the 2 data frames** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce53417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning strategy: Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e860475-557a-4a03-962e-d18784bdd630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicated raws in df_clean['ID'] \n",
    "df_clean.drop_duplicates(subset=['ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daaa603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicated raws in stats_df_clean['ID'] \n",
    "stats_df_clean.drop_duplicates(subset=['ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feaa04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the cleaning was successful\n",
    "\n",
    "# check the number of duplicated rows in dt_clean\n",
    "df_clean.duplicated(['ID']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008205be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the cleaning was successful\n",
    "\n",
    "# check the number of duplicated rows in stats_dt_clean\n",
    "stats_df_clean.duplicated(['ID']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eaeb11-38d7-4ea5-8167-dc596841b93e",
   "metadata": {},
   "source": [
    "Justification:\n",
    "\n",
    "By the investitions previously conducted, there were quite a few duplicates in both data frames. These were eliminated using the `drop_duplicted` in pandas. And now there are no duplicated entires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3984faf7-6b23-4909-9f92-d11d58a7eb0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tidiness Issue 2: \n",
    "**Date has both date and time stamps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b672305-0a20-43ce-aa82-d2d1f9dca18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning strategy: Convert data type\n",
    "\n",
    "# Convert Published_date column type to datetime format\n",
    "stats_df_clean['Published_date'] = pd.to_datetime(stats_df_clean['Published_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42afb1b0-f16b-4583-865a-c4ba4ea6bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the cleaning was successful\n",
    "print(stats_df_clean['Published_date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e215646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning strategy: Drop duplicates\n",
    "\n",
    "# Extract and display only the date \n",
    "stats_df_clean['Published_date'] = stats_df_clean.Published_date.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb232071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the cleaning was successful\n",
    "\n",
    "# print extract from stats_df to check column names\n",
    "stats_df_clean.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d634ff8-918e-44c3-a0af-cd600a96d6ef",
   "metadata": {},
   "source": [
    "Justification: \n",
    "\n",
    "For further analysis it is more useful to have the date information alone in the `Published_date` column. Therefore, the date information was extracted using `.dt.date` in pandas. However, before that the column data type needed to be changed to `datetime` format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dc3115-3f36-4fcb-9270-1e634b128c85",
   "metadata": {},
   "source": [
    "### Quality Issue 1: \n",
    "**Entries not extracting data from the API**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad742a",
   "metadata": {},
   "source": [
    "Apply the cleaning strategy:\n",
    "\n",
    "In order to target this cleaning point we will first merge the 2 datassets and check for the entries which does not get any results from the 2nd dataset. These entries will be extracted in to another dataset to further investigations. Afterwards, the empty rows will be droppped in the combined dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the 2 datasets\n",
    "final_clean_df = pd.merge(df_clean, stats_df_clean, on=['ID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd7339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sample the dataframe\n",
    "final_clean_df.sample(n=10, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5862767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows with any NaN values\n",
    "na_rows = final_clean_df[final_clean_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e82712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column with all NaN values\n",
    "na_rows = na_rows.drop(columns=['Subscribers', 'Total_vedios', 'Views', 'Published_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b4904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some NaN rows\n",
    "na_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5196f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on NaN entries\n",
    "na_rows.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN rows with NaN results in final_clean and reset index\n",
    "final_clean_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e56b536-445f-43d1-a498-cb99e62bb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the cleaning was successful\n",
    "\n",
    "# Check for null values in the dataframe\n",
    "final_clean_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8681b0-8b91-4352-b993-969239011d0b",
   "metadata": {},
   "source": [
    "Justification:\n",
    "\n",
    "As seen we were unable to get information for all the entries in the first dataframe using the APi. We were able to summarise the entries which we were not able to extarct information and save as a separate dataframe named `na_rows` for further investigation if necessary. In order to get this information we merged the two dataframe with a left merge on the df dataset and then dropped the NaN entries to construct the final dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51ff137",
   "metadata": {},
   "source": [
    "For our further data manipulations we convert the columns `Subscribers`, `Total_vedios`, and `Views` to dtype `int` and `Published_date` to `datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1879915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data types for columns\n",
    "final_clean_df = final_clean_df.astype({\"Subscribers\": int, \n",
    "                                  \"Total_vedios\": int, \n",
    "                                  \"Views\":int,\n",
    "                                  \"Published_date\": np.datetime64\n",
    "                                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da5410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset\n",
    "final_clean_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9929c1af-d8aa-4532-a238-5884bf6fb2e8",
   "metadata": {},
   "source": [
    "### **Remove unnecessary variables and combine datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9e53f7",
   "metadata": {},
   "source": [
    "We had already combined our datasets in the cleaning stage. Let's get a look at the heading of the final dataset to check if there any other fields that might be unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa991353-d5a9-4dc3-afe7-e40c3f0dc127",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the final dataset\n",
    "final_clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac6f78",
   "metadata": {},
   "source": [
    "Remove unnecessary variables and combine datasets:\n",
    " \n",
    "There are no unnecessary column in the dataset selected or the information collected for the other dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16a3b1f-5c4f-4e69-ab37-3c42b9b639aa",
   "metadata": {},
   "source": [
    "## 4. Update your data store\n",
    "Update your local database/data store with the cleaned data, following best practices for storing your cleaned data:\n",
    "\n",
    "- Must maintain different instances / versions of data (raw and cleaned data)\n",
    "- Must name the dataset files informatively\n",
    "- Ensure both the raw and cleaned data is saved to your database/data store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c91845-697e-49fd-a3af-1aa674254517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving cleaned data\n",
    "final_clean_df.to_csv('Youtube_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ea630-8c46-4216-915a-bdc9cf7a739a",
   "metadata": {},
   "source": [
    "## 5. Answer the research question\n",
    "\n",
    "### **5.1:** Define and answer the research question \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92183154",
   "metadata": {},
   "source": [
    "\n",
    "We began this project with the intention of finding the most popular Youtube channels and finding any relationships among parameters such as view counts, date of published, total vedios, total subscribers for the Youtube channels.\n",
    "\n",
    "In finding answers for these questions now we used the cleaned data we have gathers and depict them visually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bebde0-6616-471f-890d-826d220ebb00",
   "metadata": {},
   "source": [
    "**Research question: Which channels have most subscribers?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c31fe1-b477-4379-b353-cfd0313909de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visual 1 -\n",
    "\n",
    "#freq_count = final_clean.Subscribers.value_counts()\n",
    "top_channels = final_clean_df.nlargest(20, 'Subscribers');\n",
    "\n",
    "# Assign plot size\n",
    "plt.figure(figsize = [10,5]);\n",
    "\n",
    "# Draw plot\n",
    "top_channels.plot(kind='bar', x='Name', y='Subscribers', color='skyblue')\n",
    "plt.title('Subscribers vs. Channel')\n",
    "plt.xlabel('Channel Name')\n",
    "plt.ylabel('Subscribers');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e3319-c589-490c-b69e-35e1141c1c3a",
   "metadata": {},
   "source": [
    "Answer to research question:\n",
    "\n",
    "* According to the bar chart above, the channels, MrBeast (~300million), T-series (~270million) and Cocomelon (~180million) seems to be the most popular channels in Youtube as they have the most subscribers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032816b6",
   "metadata": {},
   "source": [
    "**Research question: Are there any relationship present between `Subscribers`, `Total_vedios`, `Views`, and  `Published_date` in this dataset?**\n",
    "\n",
    "In order to look into this aspect we construct a PairGrid as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a1e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual 2 \n",
    "\n",
    "# draw PairGrid\n",
    "g = sns.PairGrid(data=final_clean_df, vars=['Subscribers', 'Total_vedios', 'Views', 'Published_date'])\n",
    "g.map_diag(plt.hist)\n",
    "g.map_offdiag(plt.scatter);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289fe6ce",
   "metadata": {},
   "source": [
    "Only `Subscribers` vs `Views` show a linear relationship from the above data. Therefore, we redraw a scatter plot for these 2 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual 3 \n",
    "\n",
    "# Assign plot size\n",
    "plt.figure(figsize = [6,6]);\n",
    "\n",
    "# Draw scatter plot\n",
    "sns.scatterplot(data=final_clean_df, x='Subscribers', y='Views');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183809da-5e0f-4f27-830c-4128487dc952",
   "metadata": {},
   "source": [
    "Answer to research question: \n",
    "\n",
    "* According to the PairGrid it was clear that only `Subscribers` and `Views` showed a linear rlationship. Hence, more the subscribers a channel will have it will get more views no matter the Published date of the channel or the total number of vedios it might carry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137dd657-10a8-40ba-941e-2eb60d68d0c8",
   "metadata": {},
   "source": [
    "### **5.2:** Reflection\n",
    "In 2-4 sentences, if you had more time to complete the project, what actions would you take? For example, which data quality and structural issues would you look into further, and what research questions would you further explore?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c93607-eda4-49c1-b3eb-d45a5926fd1b",
   "metadata": {},
   "source": [
    "*Answer:* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acc580a",
   "metadata": {},
   "source": [
    "* An extension of this project I would be to look into in depth at the channels which did not extract any information using the API. There could be reasons such as inaccurate channel IDs, the channel not being currently present or another reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b8dd6-0dd5-49ea-8f5d-40e3ea8b55e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
